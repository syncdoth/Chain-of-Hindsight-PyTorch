{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "from coh.data import CoHDataset\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/synch/opt/miniconda3/envs/torch-m1/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "config = CoHDataset.get_default_config()\n",
    "# NOTE: t5-small is not actually supported in this repo!!!!\n",
    "tokenizer = AutoTokenizer.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset webgpt_comparisons (/Users/synch/.cache/huggingface/datasets/openai___webgpt_comparisons/default/0.0.0/8b5d5879cdc98c4c0099af6053dffe8d504588d43d3b11f1b1ec223ab1e8db0a)\n",
      "Using custom data configuration Anthropic--hh-rlhf-c8cd8dc58ab67414\n",
      "Found cached dataset json (/Users/synch/.cache/huggingface/datasets/Anthropic___json/Anthropic--hh-rlhf-c8cd8dc58ab67414/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
      "Found cached dataset summarize_from_feedback (/Users/synch/.cache/huggingface/datasets/openai___summarize_from_feedback/comparisons/0.0.0/483f970ceb55b926b0a087ef4f678ab1b089bc8174a107a452c6152e88af7ff0)\n"
     ]
    }
   ],
   "source": [
    "data = CoHDataset(config, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hf_tokens': tensor([[ 2875,    11,  2721,  ...,     5,  3892,    10],\n",
      "        [ 2840,    54,    27,  ...,    17,     5,   189],\n",
      "        [    2,     5,   354,  ...,     7,   114,    46],\n",
      "        ...,\n",
      "        [  417,     3,    99,  ..., 24783,     3,  8607],\n",
      "        [   27,   470,   243,  ...,    28,    34,    57],\n",
      "        [ 3874,     8,  5779,  ...,    58,  3892,    10]]), 'hf_masks': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'pt_tokens': tensor([[   30,    39,  6089,  ..., 16002,    26,  7059],\n",
      "        [ 3939,     5,   100,  ...,     3,    58,     3],\n",
      "        [   18,   465,     6,  ...,    21,    17,  7602],\n",
      "        ...,\n",
      "        [   21,    48,   562,  ...,    27,   410,    59],\n",
      "        [ 2085,    44,    66,  ...,  4738,  5377,     7],\n",
      "        [    5,   784, 13026,  ...,   547,     5,   784]])}\n",
      "{'hf_tokens': torch.Size([8, 512]), 'hf_masks': torch.Size([8, 512]), 'pt_tokens': torch.Size([8, 512])}\n"
     ]
    }
   ],
   "source": [
    "batch = next(data_iter)\n",
    "print(batch)\n",
    "print({k: v.shape for k, v in batch.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
